{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8feae029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f41f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_small_h5(input_path, output_path, sample_size=500, group_name=\"dataset2\", seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        with h5py.File(output_path, \"w\") as fout:\n",
    "            # Crear estructura\n",
    "            fin_group = fin[group_name]\n",
    "            fout_group = fout.create_group(group_name)\n",
    "            fout_k = fout_group.create_group(\"keypoints\")\n",
    "            fout_e = fout_group.create_group(\"embeddings\")\n",
    "            fout_l = fout_group.create_group(\"labels\") if \"labels\" in fin_group else None\n",
    "\n",
    "            # Obtener clips válidos\n",
    "            clips = list(fin_group[\"keypoints\"].keys())\n",
    "            random.shuffle(clips)\n",
    "            selected = clips[:sample_size]\n",
    "\n",
    "            for clip in selected:\n",
    "                # Copiar keypoints\n",
    "                data = fin_group[\"keypoints\"][clip][:]\n",
    "                fout_k.create_dataset(clip, data=data, compression=\"gzip\")\n",
    "\n",
    "                # Copiar embedding\n",
    "                emb = fin_group[\"embeddings\"][clip][:]\n",
    "                fout_e.create_dataset(clip, data=emb)\n",
    "\n",
    "                # Copiar label si hay\n",
    "                if fout_l:\n",
    "                    label = fin_group[\"labels\"][clip][:]\n",
    "                    fout_l.create_dataset(clip, data=label)\n",
    "\n",
    "    print(f\"✅ Guardado {sample_size} clips en '{output_path}'\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# make_small_h5(\"dataset_grande.h5\", \"dataset_pequeño.h5\", sample_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6709e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_small_h5_ratio(input_path, output_path, sample_ratio=0.05, group_name=\"dataset2\", seed=42):\n",
    "    \"\"\"\n",
    "    Crea un .hdf5 reducido tomando una fracción aleatoria de los clips.\n",
    "    - sample_ratio: proporción de clips a conservar (ej: 0.05 para 5%)\n",
    "    \"\"\"\n",
    "    import h5py, random\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        with h5py.File(output_path, \"w\") as fout:\n",
    "            fin_group = fin[group_name]\n",
    "            fout_group = fout.create_group(group_name)\n",
    "            fout_k = fout_group.create_group(\"keypoints\")\n",
    "            fout_e = fout_group.create_group(\"embeddings\")\n",
    "            fout_l = fout_group.create_group(\"labels\") if \"labels\" in fin_group else None\n",
    "\n",
    "            # Lista de clips\n",
    "            clips = list(fin_group[\"keypoints\"].keys())\n",
    "            total = len(clips)\n",
    "            n = max(1, int(total * sample_ratio))\n",
    "\n",
    "            print(f\"➡️  Tomando {n} de {total} clips ({sample_ratio*100:.1f}%)\")\n",
    "\n",
    "            selected = random.sample(clips, n)\n",
    "\n",
    "            for clip in selected:\n",
    "                fout_k.create_dataset(clip, data=fin_group[\"keypoints\"][clip][:], compression=\"gzip\")\n",
    "                fout_e.create_dataset(clip, data=fin_group[\"embeddings\"][clip][:])\n",
    "                if fout_l:\n",
    "                    fout_l.create_dataset(clip, data=fin_group[\"labels\"][clip][:])\n",
    "\n",
    "    print(f\"✅ Guardado en '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45f0fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyfiles.ipynb  dataset6-video.py\t   dataset_small50.hdf5\n",
      "dataset1\t dataset7\t\t   DataverseDownload.ipynb\n",
      "dataset1.hdf5\t dataset_clean_clean.hdf5  note.md\n",
      "dataset2\t dataset_clean.hdf5\t   script.sh\n",
      "dataset3\t dataset.hdf5\t\t   srt_videos.ipynb\n",
      "dataset4\t dataset_small1.hdf5\t   transcripcion.ipynb\n",
      "dataset5\t dataset_small25.hdf5\n",
      "dataset6\t dataset_small2.hdf5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = \"../../../../data/dataset_clean_clean.hdf5\"\n",
    "sample_ratio = 25\n",
    "output_path = f\"../../../../data/dataset_small_clean{sample_ratio}.hdf5\"\n",
    "!dir ../../../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7693f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Tomando 2114 de 8459 clips (25.0%)\n",
      "✅ Guardado en '../../../../data/dataset_small_clean25.hdf5'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#make_small_h5(data_path, output_path, sample_size=sample_size, group_name=\"dataset2\", seed=42)\n",
    "make_small_h5_ratio(data_path, output_path, sample_ratio=sample_ratio/100, group_name=\"dataset2\", seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c336db5",
   "metadata": {},
   "source": [
    "# clean dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_keypoinys(keypoints):\n",
    "    T, N, _ = keypoints.shape\n",
    "    filtered = keypoints[:, 117:, :].clone()  # Clonar para evitar modificar el original\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import as_tensor\n",
    "import torch\n",
    "\n",
    "def clean_fn(keypoints_np):\n",
    "    keypoints = as_tensor(keypoints_np, dtype=torch.float32)\n",
    "    cleaned = fix_keypoinys(keypoints)\n",
    "    return cleaned.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2064a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clean_h5_all_groups(input_path, output_path, clean_keypoints_fn=None):\n",
    "    \"\"\"\n",
    "    Crea un nuevo archivo HDF5 copiando todos los grupos y clips del original,\n",
    "    aplicando una función de limpieza a los keypoints.\n",
    "\n",
    "    - clean_keypoints_fn: función que recibe un array (T, J, 2) y retorna el keypoint limpio.\n",
    "    \"\"\"\n",
    "    import h5py\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        with h5py.File(output_path, \"w\") as fout:\n",
    "            group_names = list(fin.keys())\n",
    "            print(f\"📁 Grupos encontrados: {group_names}\")\n",
    "\n",
    "            for group_name in group_names:\n",
    "                print(f\"\\n➡️ Procesando grupo '{group_name}'...\")\n",
    "                fin_group = fin[group_name]\n",
    "                fout_group = fout.create_group(group_name)\n",
    "\n",
    "                fout_k = fout_group.create_group(\"keypoints\")\n",
    "                fout_e = fout_group.create_group(\"embeddings\")\n",
    "                fout_l = fout_group.create_group(\"labels\") if \"labels\" in fin_group else None\n",
    "\n",
    "                clips = list(fin_group[\"keypoints\"].keys())\n",
    "                print(f\"  🧩 {len(clips)} clips\")\n",
    "\n",
    "                for clip in clips:\n",
    "                    keypoints = fin_group[\"keypoints\"][clip][:]\n",
    "                    if clean_keypoints_fn:\n",
    "                        keypoints = clean_keypoints_fn(keypoints)\n",
    "\n",
    "                    fout_k.create_dataset(clip, data=keypoints, compression=\"gzip\")\n",
    "                    fout_e.create_dataset(clip, data=fin_group[\"embeddings\"][clip][:])\n",
    "                    if fout_l:\n",
    "                        fout_l.create_dataset(clip, data=fin_group[\"labels\"][clip][:])\n",
    "\n",
    "    print(f\"\\n✅ Dataset limpio guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_clean_h5_all_groups(\n",
    "    input_path=\"../../../../data/dataset1.hdf5\",\n",
    "    output_path=\"../../../../data/dataset_clean.hdf5\",\n",
    "    clean_keypoints_fn=clean_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def extract_kp0_variance(input_path):\n",
    "    \"\"\"\n",
    "    Recorre un HDF5 con estructura (grupo -> clips -> keypoints) y \n",
    "    calcula la varianza temporal del keypoint 0 por clip.\n",
    "    \"\"\"\n",
    "    registros = []\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        for group_name in fin.keys():\n",
    "            group = fin[group_name]\n",
    "            for clip_name in group[\"keypoints\"].keys():\n",
    "                keypoints = group[\"keypoints\"][clip_name][:]  # shape: (T, J, 2)\n",
    "\n",
    "                if keypoints.shape[1] <= 0:\n",
    "                    continue  # skip si no hay joints\n",
    "\n",
    "                kp0 = keypoints[:, 0, :]  # (T, 2)\n",
    "                var_x = np.var(kp0[:, 0])\n",
    "                var_y = np.var(kp0[:, 1])\n",
    "                var_total = var_x + var_y\n",
    "\n",
    "                registros.append({\n",
    "                    \"grupo\": group_name,\n",
    "                    \"clip\": clip_name,\n",
    "                    \"var_x\": var_x,\n",
    "                    \"var_y\": var_y,\n",
    "                    \"var_total\": var_total\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(registros)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = extract_kp0_variance(\"../../../../data/dataset_clean.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Percentil 95 para decidir max_seq_len sin truncar tanto\n",
    "p95 = int(np.percentile(df['var_x'], 80))\n",
    "\n",
    "# Visualización\n",
    "plt.hist(df['var_x'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.axvline(p95, color='red', linestyle='--', label=f'95% ≤ {p95}')\n",
    "plt.title(\"Distribución de longitudes de secuencia\")\n",
    "plt.xlabel(\"Frames por muestra\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_side_dominance(keypoints, threshold=0.6):\n",
    "    \"\"\"\n",
    "    keypoints: (T, J, 2)\n",
    "    Retorna \"left\" o \"right\" según dónde pasó más tiempo el keypoint 0.\n",
    "    \"\"\"\n",
    "    kp0_x = keypoints[:, 0, 0]  # eje x del keypoint 0\n",
    "    mid_x = np.median(kp0_x)  # usar mediana como frontera\n",
    "\n",
    "    left_count = np.sum(kp0_x < mid_x)\n",
    "    right_count = np.sum(kp0_x >= mid_x)\n",
    "    total = len(kp0_x)\n",
    "\n",
    "    if left_count / total >= threshold:\n",
    "        return \"left\"\n",
    "    elif right_count / total >= threshold:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"mixed\"\n",
    "\n",
    "def clean_by_dominant_side(keypoints, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Elimina frames donde el keypoint 0 cambia de lado respecto al dominante.\n",
    "    \"\"\"\n",
    "    kp0_x = keypoints[:, 0, 0]\n",
    "    mid_x = np.median(kp0_x)\n",
    "    dominant_side = detect_side_dominance(keypoints, threshold)\n",
    "\n",
    "    if dominant_side == \"left\":\n",
    "        mask = kp0_x < mid_x\n",
    "    elif dominant_side == \"right\":\n",
    "        mask = kp0_x >= mid_x\n",
    "    else:\n",
    "        return keypoints  # no filtramos si es mixto\n",
    "\n",
    "    return keypoints[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ffc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints_comparison(original, filtrado, joint_id=0):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    fig.suptitle(f\"Comparación del keypoint {joint_id} (original vs filtrado)\", fontsize=14)\n",
    "\n",
    "    ax[0].plot(original[:, joint_id, 0], original[:, joint_id, 1], 'bo-', alpha=0.5)\n",
    "    ax[0].set_title(f\"Original - {original.shape[0]} frames\")\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].grid(True)\n",
    "    ax[0].set_aspect(\"equal\")\n",
    "\n",
    "    ax[1].plot(filtrado[:, joint_id, 0], filtrado[:, joint_id, 1], 'ro-', alpha=0.5)\n",
    "    ax[1].set_title(f\"Filtrado - {filtrado.shape[0]} frames\")\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].grid(True)\n",
    "    ax[1].set_aspect(\"equal\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../../data/dataset_clean.hdf5\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "# ------------ Filtros ------------\n",
    "\n",
    "def detect_side_dominance(keypoints, threshold=0.6):\n",
    "    kp0_x = keypoints[:, 0, 0]\n",
    "    mid_x = np.median(kp0_x)\n",
    "    left = np.sum(kp0_x < mid_x)\n",
    "    right = np.sum(kp0_x >= mid_x)\n",
    "    return \"left\" if left/len(kp0_x) >= threshold else \"right\" if right/len(kp0_x) >= threshold else \"mixed\"\n",
    "\n",
    "def clean_by_dominant_side(keypoints, threshold=0.6):\n",
    "    kp0_x = keypoints[:, 0, 0]\n",
    "    mid_x = np.median(kp0_x)\n",
    "    side = detect_side_dominance(keypoints, threshold)\n",
    "    if side == \"left\":\n",
    "        return keypoints[kp0_x < mid_x]\n",
    "    elif side == \"right\":\n",
    "        return keypoints[kp0_x >= mid_x]\n",
    "    return keypoints\n",
    "\n",
    "def extract_stable_segment(keypoints, margin=15):\n",
    "    T = keypoints.shape[0]\n",
    "    mask = np.zeros(T, dtype=bool)\n",
    "    for t in range(1, T):\n",
    "        x1 = keypoints[t-1, 1, 0]\n",
    "        x4 = keypoints[t-1, 4, 0]\n",
    "        x_min = min(x1, x4) - margin\n",
    "        x_max = max(x1, x4) + margin\n",
    "        head_x = keypoints[t, 0, 0]\n",
    "        if x_min <= head_x <= x_max:\n",
    "            mask[t] = True\n",
    "\n",
    "    indices = np.where(mask)[0]\n",
    "    groups = [list(map(itemgetter(1), g)) for k, g in groupby(enumerate(indices), lambda i: i[0]-i[1])]\n",
    "    return keypoints[max(groups, key=len)] if groups else None\n",
    "\n",
    "def clean_keypoints_smart(keypoints, min_len=30):\n",
    "    stable = extract_stable_segment(keypoints)\n",
    "    if stable is not None and len(stable) >= min_len:\n",
    "        return stable, \"stable\"\n",
    "    return clean_by_dominant_side(keypoints), \"dominant\"\n",
    "\n",
    "# ------------ Visualización ------------\n",
    "\n",
    "def visualize_first_clip(file_path):\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        group = list(f.keys())[0]\n",
    "        clip = list(f[group][\"keypoints\"].keys())[0]\n",
    "        keypoints = f[group][\"keypoints\"][clip][:]\n",
    "\n",
    "    filtered, method = clean_keypoints_smart(keypoints)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"frame\": np.arange(len(keypoints)),\n",
    "        \"kp0_x_original\": keypoints[:, 0, 0],\n",
    "        \"kp0_x_filtered\": np.nan\n",
    "    })\n",
    "\n",
    "    filtered_x = set(filtered[:, 0, 0])\n",
    "    mask = np.isin(keypoints[:, 0, 0], list(filtered_x))\n",
    "    df.loc[mask, \"kp0_x_filtered\"] = keypoints[mask, 0, 0]\n",
    "\n",
    "    # Graficar\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(df[\"frame\"], df[\"kp0_x_original\"], label=\"Original\", alpha=0.5)\n",
    "    plt.plot(df[\"frame\"], df[\"kp0_x_filtered\"], label=\"Filtrado\", alpha=0.8)\n",
    "    plt.title(f\"Keypoint 0 - Eje X | Método: {method}\")\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(\"Posición X\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ------------ Uso ------------\n",
    "\n",
    "# Reemplaza esto por tu archivo\n",
    "df_cmp = visualize_first_clip(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa39c2f",
   "metadata": {},
   "source": [
    "Ahora con esto podemos ver si hay un cambio abrupto y determinar que el open pose cambio de foco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a1f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "# ---------- Interpolador ----------\n",
    "\n",
    "def interpolate_unstable_frames(keypoints, margin=15):\n",
    "    keypoints_interp = keypoints.copy()\n",
    "    T = keypoints.shape[0]\n",
    "    valid_mask = np.zeros(T, dtype=bool)\n",
    "\n",
    "    for t in range(1, T):\n",
    "        x1 = keypoints[t-1, 1, 0]\n",
    "        x4 = keypoints[t-1, 4, 0]\n",
    "        x_min = min(x1, x4) - margin\n",
    "        x_max = max(x1, x4) + margin\n",
    "        head_x = keypoints[t, 0, 0]\n",
    "        if x_min <= head_x <= x_max:\n",
    "            valid_mask[t] = True\n",
    "    valid_mask[0] = True\n",
    "\n",
    "    invalid_idx = np.where(~valid_mask)[0]\n",
    "    valid_idx = np.where(valid_mask)[0]\n",
    "\n",
    "    for i in invalid_idx:\n",
    "        before = valid_idx[valid_idx < i]\n",
    "        after = valid_idx[valid_idx > i]\n",
    "        if len(before) == 0 or len(after) == 0:\n",
    "            continue\n",
    "        t0 = before[-1]\n",
    "        t1 = after[0]\n",
    "        alpha = (i - t0) / (t1 - t0)\n",
    "        keypoints_interp[i] = (1 - alpha) * keypoints[t0] + alpha * keypoints[t1]\n",
    "\n",
    "    return keypoints_interp\n",
    "\n",
    "# ---------- Adaptación de limpieza + tracking varianza ----------\n",
    "\n",
    "def make_clean_h5_all_groups_with_stats(input_path, output_path, clean_keypoints_fn=None):\n",
    "    df_before = []\n",
    "    df_after = []\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        with h5py.File(output_path, \"w\") as fout:\n",
    "            for group_name in fin.keys():\n",
    "                fin_group = fin[group_name]\n",
    "                fout_group = fout.create_group(group_name)\n",
    "\n",
    "                fout_k = fout_group.create_group(\"keypoints\")\n",
    "                fout_e = fout_group.create_group(\"embeddings\")\n",
    "                fout_l = fout_group.create_group(\"labels\") if \"labels\" in fin_group else None\n",
    "\n",
    "                for clip in fin_group[\"keypoints\"].keys():\n",
    "                    keypoints = fin_group[\"keypoints\"][clip][:]\n",
    "                    var_before = np.var(keypoints[:, 0, :], axis=0).sum()\n",
    "                    df_before.append({\n",
    "                        \"group\": group_name,\n",
    "                        \"clip\": clip,\n",
    "                        \"var_x\": np.var(keypoints[:, 0, 0]),\n",
    "                        \"var_y\": np.var(keypoints[:, 0, 1]),\n",
    "                        \"var_total\": var_before\n",
    "                    })\n",
    "\n",
    "                    if clean_keypoints_fn:\n",
    "                        keypoints = clean_keypoints_fn(keypoints)\n",
    "\n",
    "                    var_after = np.var(keypoints[:, 0, :], axis=0).sum()\n",
    "                    df_after.append({\n",
    "                        \"group\": group_name,\n",
    "                        \"clip\": clip,\n",
    "                        \"var_x\": np.var(keypoints[:, 0, 0]),\n",
    "                        \"var_y\": np.var(keypoints[:, 0, 1]),\n",
    "                        \"var_total\": var_after\n",
    "                    })\n",
    "\n",
    "                    fout_k.create_dataset(clip, data=keypoints, compression=\"gzip\")\n",
    "                    fout_e.create_dataset(clip, data=fin_group[\"embeddings\"][clip][:])\n",
    "                    if fout_l:\n",
    "                        fout_l.create_dataset(clip, data=fin_group[\"labels\"][clip][:])\n",
    "\n",
    "    return pd.DataFrame(df_before), pd.DataFrame(df_after)\n",
    "\n",
    "# Simularemos ruta de uso\n",
    "i_path = \"../../../../data/dataset_clean.hdf5\"\n",
    "o_path = \"../../../../data/dataset_clean_clean.hdf5\"\n",
    "\n",
    "df_before, df_after = make_clean_h5_all_groups_with_stats(i_path, o_path, interpolate_unstable_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = df_before.copy()\n",
    "df_diff[\"var_total_after\"] = df_after[\"var_total\"]\n",
    "df_diff[\"delta_var\"] = df_diff[\"var_total\"] - df_diff[\"var_total_after\"]\n",
    "df_diff[\"rel_reduction_%\"] = 100 * df_diff[\"delta_var\"] / df_diff[\"var_total\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Antes (total):\", df_before[\"var_total\"].sum())\n",
    "print(\"Después (total):\", df_after[\"var_total\"].sum())\n",
    "print(\"Reducción total (%):\", 100 * (df_before[\"var_total\"].sum() - df_after[\"var_total\"].sum()) / df_before[\"var_total\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores\n",
    "print(\"🔽 Mayor reducción de varianza:\")\n",
    "df_diff.sort_values(\"delta_var\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Empeoramientos raros\n",
    "print(\"🔼 Clips donde subió la varianza (deberían ser pocos o ninguno):\")\n",
    "df_diff[df_diff[\"delta_var\"] < 0].sort_values(\"delta_var\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(df_diff[\"var_total\"], df_diff[\"var_total_after\"], alpha=0.5)\n",
    "plt.plot([df_diff[\"var_total\"].min(), df_diff[\"var_total\"].max()],\n",
    "         [df_diff[\"var_total\"].min(), df_diff[\"var_total\"].max()],\n",
    "         linestyle='--', color='gray')\n",
    "plt.xlabel(\"Varianza antes\")\n",
    "plt.ylabel(\"Varianza después\")\n",
    "plt.title(\"Comparación de varianza total por clip (keypoint 0)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62eab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "pivot = df_diff.pivot_table(index=\"group\", values=[\"var_total\", \"var_total_after\"], aggfunc=\"mean\")\n",
    "pivot[\"rel_reduction_%\"] = 100 * (pivot[\"var_total\"] - pivot[\"var_total_after\"]) / pivot[\"var_total\"]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.heatmap(pivot[[\"rel_reduction_%\"]], annot=True, fmt=\".1f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Reducción promedio de varianza por grupo (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a951b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏆 Top clips con mayor reducción total:\")\n",
    "df_diff.sort_values(\"delta_var\", ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
