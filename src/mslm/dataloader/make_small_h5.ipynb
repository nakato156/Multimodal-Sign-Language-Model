{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8feae029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f41f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_small_h5(input_path, output_path, sample_size=500, group_name=\"dataset2\", seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        with h5py.File(output_path, \"w\") as fout:\n",
    "            # Crear estructura\n",
    "            fin_group = fin[group_name]\n",
    "            fout_group = fout.create_group(group_name)\n",
    "            fout_k = fout_group.create_group(\"keypoints\")\n",
    "            fout_e = fout_group.create_group(\"embeddings\")\n",
    "            fout_l = fout_group.create_group(\"labels\") if \"labels\" in fin_group else None\n",
    "\n",
    "            # Obtener clips v√°lidos\n",
    "            clips = list(fin_group[\"keypoints\"].keys())\n",
    "            random.shuffle(clips)\n",
    "            selected = clips[:sample_size]\n",
    "\n",
    "            for clip in selected:\n",
    "                # Copiar keypoints\n",
    "                data = fin_group[\"keypoints\"][clip][:]\n",
    "                fout_k.create_dataset(clip, data=data, compression=\"gzip\")\n",
    "\n",
    "                # Copiar embedding\n",
    "                emb = fin_group[\"embeddings\"][clip][:]\n",
    "                fout_e.create_dataset(clip, data=emb)\n",
    "\n",
    "                # Copiar label si hay\n",
    "                if fout_l:\n",
    "                    label = fin_group[\"labels\"][clip][:]\n",
    "                    fout_l.create_dataset(clip, data=label)\n",
    "\n",
    "    print(f\"‚úÖ Guardado {sample_size} clips en '{output_path}'\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# make_small_h5(\"dataset_grande.h5\", \"dataset_peque√±o.h5\", sample_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6709e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_small_h5_ratio(input_path, output_path, sample_ratio=0.05, group_name=\"dataset2\", seed=42):\n",
    "    \"\"\"\n",
    "    Crea un .hdf5 reducido tomando una fracci√≥n aleatoria de los clips.\n",
    "    - sample_ratio: proporci√≥n de clips a conservar (ej: 0.05 para 5%)\n",
    "    \"\"\"\n",
    "    import h5py, random\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        with h5py.File(output_path, \"w\") as fout:\n",
    "            fin_group = fin[group_name]\n",
    "            fout_group = fout.create_group(group_name)\n",
    "            fout_k = fout_group.create_group(\"keypoints\")\n",
    "            fout_e = fout_group.create_group(\"embeddings\")\n",
    "            fout_l = fout_group.create_group(\"labels\") if \"labels\" in fin_group else None\n",
    "\n",
    "            # Lista de clips\n",
    "            clips = list(fin_group[\"keypoints\"].keys())\n",
    "            total = len(clips)\n",
    "            n = max(1, int(total * sample_ratio))\n",
    "\n",
    "            print(f\"‚û°Ô∏è  Tomando {n} de {total} clips ({sample_ratio*100:.1f}%)\")\n",
    "\n",
    "            selected = random.sample(clips, n)\n",
    "\n",
    "            for clip in selected:\n",
    "                fout_k.create_dataset(clip, data=fin_group[\"keypoints\"][clip][:], compression=\"gzip\")\n",
    "                fout_e.create_dataset(clip, data=fin_group[\"embeddings\"][clip][:])\n",
    "                if fout_l:\n",
    "                    fout_l.create_dataset(clip, data=fin_group[\"labels\"][clip][:])\n",
    "\n",
    "    print(f\"‚úÖ Guardado en '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45f0fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è  Tomando 2114 de 8459 clips (25.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado en '../../../../data/dataset_small25.hdf5'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = \"../../../../data/dataset.hdf5\"\n",
    "sample_ratio = 25\n",
    "output_path = f\"../../../../data/dataset_small{sample_ratio}.hdf5\"\n",
    "\n",
    "#make_small_h5(data_path, output_path, sample_size=sample_size, group_name=\"dataset2\", seed=42)\n",
    "make_small_h5_ratio(data_path, output_path, sample_ratio=sample_ratio/100, group_name=\"dataset2\", seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c336db5",
   "metadata": {},
   "source": [
    "# clean dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_keypoinys(keypoints):\n",
    "    T, N, _ = keypoints.shape\n",
    "    filtered = keypoints[:, 117:, :].clone()  # Clonar para evitar modificar el original\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3f7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import as_tensor\n",
    "import torch\n",
    "\n",
    "def clean_fn(keypoints_np):\n",
    "    keypoints = as_tensor(keypoints_np, dtype=torch.float32)\n",
    "    cleaned = fix_keypoinys(keypoints)\n",
    "    return cleaned.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2064a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clean_h5_all_groups(input_path, output_path, clean_keypoints_fn=None):\n",
    "    \"\"\"\n",
    "    Crea un nuevo archivo HDF5 copiando todos los grupos y clips del original,\n",
    "    aplicando una funci√≥n de limpieza a los keypoints.\n",
    "\n",
    "    - clean_keypoints_fn: funci√≥n que recibe un array (T, J, 2) y retorna el keypoint limpio.\n",
    "    \"\"\"\n",
    "    import h5py\n",
    "\n",
    "    with h5py.File(input_path, \"r\") as fin:\n",
    "        with h5py.File(output_path, \"w\") as fout:\n",
    "            group_names = list(fin.keys())\n",
    "            print(f\"üìÅ Grupos encontrados: {group_names}\")\n",
    "\n",
    "            for group_name in group_names:\n",
    "                print(f\"\\n‚û°Ô∏è Procesando grupo '{group_name}'...\")\n",
    "                fin_group = fin[group_name]\n",
    "                fout_group = fout.create_group(group_name)\n",
    "\n",
    "                fout_k = fout_group.create_group(\"keypoints\")\n",
    "                fout_e = fout_group.create_group(\"embeddings\")\n",
    "                fout_l = fout_group.create_group(\"labels\") if \"labels\" in fin_group else None\n",
    "\n",
    "                clips = list(fin_group[\"keypoints\"].keys())\n",
    "                print(f\"  üß© {len(clips)} clips\")\n",
    "\n",
    "                for clip in clips:\n",
    "                    keypoints = fin_group[\"keypoints\"][clip][:]\n",
    "                    if clean_keypoints_fn:\n",
    "                        keypoints = clean_keypoints_fn(keypoints)\n",
    "                        print(f\"  üîß Keypoints limpiados para clip '{clip} con\")\n",
    "\n",
    "                    fout_k.create_dataset(clip, data=keypoints, compression=\"gzip\")\n",
    "                    fout_e.create_dataset(clip, data=fin_group[\"embeddings\"][clip][:])\n",
    "                    if fout_l:\n",
    "                        fout_l.create_dataset(clip, data=fin_group[\"labels\"][clip][:])\n",
    "\n",
    "    print(f\"\\n‚úÖ Dataset limpio guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b4598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Grupos encontrados: ['dataset1', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7']\n",
      "\n",
      "‚û°Ô∏è Procesando grupo 'dataset1'...\n",
      "  üß© 3200 clips\n"
     ]
    }
   ],
   "source": [
    "\n",
    "make_clean_h5_all_groups(\n",
    "    input_path=\"../../../../data/dataset1.hdf5\",\n",
    "    output_path=\"../../../../data/dataset_clean.hdf5\",\n",
    "    clean_keypoints_fn=clean_fn\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
