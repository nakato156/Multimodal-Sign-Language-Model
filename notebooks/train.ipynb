{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath(os.getcwd())), \"src\"))\n",
    "os.chdir(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.mslm.utils.setup_train import setup_paths\n",
    "from src.mslm.utils import create_dataloaders, build_model, run_training, prepare_datasets, ConfigLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\t10191\n",
      "Validation size:\t2547\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "_, _, h5_file = setup_paths()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_parameters = ConfigLoader(\"config/model/config.toml\").load_config()\n",
    "model_parameters.update({\n",
    "    \"device\": device if model_parameters.get(\"device\") == \"auto\" else model_parameters.get(\"device\", device),\n",
    "    \"input_size\": 250 * 2,\n",
    "    \"output_size\": 3072,\n",
    "})\n",
    "\n",
    "# --- parametros de entrenamiento ---\n",
    "train_ratio = 0.8\n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "checkpoint_interval = 5\n",
    "log_interval = 2\n",
    "\n",
    "# --- config de entrenamiento ---\n",
    "train_config = ConfigLoader(\"config/training/train_config.toml\").load_config()\n",
    "train_ratio = train_config.get(\"train_ratio\", train_ratio)\n",
    "train_config.update({\n",
    "    \"model_version\": 1000,\n",
    "    \"learning_rate\": train_config.get(\"learning_rate\", 0.00238),\n",
    "    \"epochs\": epochs if epochs else train_config.get(\"epochs\", 100),\n",
    "    \"batch_size\": batch_size if batch_size else train_config.get(\"batch_size\", 32),\n",
    "    \"checkpoint_interval\": checkpoint_interval if checkpoint_interval else train_config.get(\"checkpoint_interval\", 5),\n",
    "    \"log_interval\": log_interval if log_interval else train_config.get(\"log_interval\", 2),\n",
    "    \"train_ratio\": train_ratio,\n",
    "    \"validation_ratio\": round(1 - train_ratio, 2),\n",
    "    \"device\": device if model_parameters.get(\"device\") == \"auto\" else model_parameters.get(\"device\", device),\n",
    "})\n",
    "    \n",
    "tr_ds, val_ds = prepare_datasets(h5_file, train_ratio)\n",
    "\n",
    "# Seleccionamos solo los primeros 2 ítems de cada dataset para pruebas rápidas\n",
    "train_subset = Subset(tr_ds, list(range(2)))\n",
    "val_subset = Subset(val_ds, list(range(2)))\n",
    "\n",
    "tr_dl, val_dl = create_dataloaders(train_subset, val_subset, batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3072])\n"
     ]
    }
   ],
   "source": [
    "for a in tr_dl:\n",
    "    print(a[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88, 250, 2]) torch.Size([3, 3072])\n",
      "torch.Size([118, 250, 2]) torch.Size([4, 3072])\n"
     ]
    }
   ],
   "source": [
    "for a in train_subset:\n",
    "    print(a[0].shape, a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): Imitator(\n",
      "    (linear_feat): Sequential(\n",
      "      (0): Linear(in_features=500, out_features=512, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (4): GELU(approximate='none')\n",
      "      (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (act1): GELU(approximate='none')\n",
      "    (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "    (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (act2): GELU(approximate='none')\n",
      "    (linear_hidden): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (pe): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.4, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.4, inplace=False)\n",
      "          (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=512, out_features=3072, bias=True)\n",
      "    (cross_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n",
      "    )\n",
      "    (norm_attn): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "    (proj_final): Sequential(\n",
      "      (0): Linear(in_features=3072, out_features=6144, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=6144, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "85.12 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = build_model(**model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Starting training...\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   0%|\u001b[32m          \u001b[0m| 0/200 [00:00<?, ?it/s]W0626 01:09:17.041000 1251143 site-packages/torch/_logging/_internal.py:1089] [7/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "Entrenando:   0%|\u001b[32m          \u001b[0m| 0/200 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0.\t Total loss: 2.811551809310913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   0%|\u001b[32m          \u001b[0m| 1/200 [00:08<29:06,  8.78s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.216034173965454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   1%|\u001b[32m          \u001b[0m| 2/200 [00:11<17:36,  5.33s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   2%|\u001b[32m▏         \u001b[0m| 3/200 [00:13<11:50,  3.60s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2.\t Total loss: 2.756175994873047\n",
      "Validation loss: 3.1631062030792236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   2%|\u001b[32m▏         \u001b[0m| 4/200 [00:14<08:48,  2.70s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1] Graph break from `Tensor.item()`, consider setting:\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1] or:\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1] to include these operations in the captured graph.\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1] \n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1] Graph break: from user code at:\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1]   File \"/home/giorgio6846/Code/Sign-AI/Sign-chris/src/mslm/training/trainer.py\", line 84, in linear_warmup_cosine_decay\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1]     ).item()\n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1] \n",
      "W0626 01:09:28.332000 1251143 site-packages/torch/_dynamo/variables/tensor.py:869] [11/1] \n",
      "Entrenando:   2%|\u001b[32m▎         \u001b[0m| 5/200 [00:16<07:25,  2.28s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4.\t Total loss: 2.437065839767456\n",
      "Validation loss: 3.034444570541382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   3%|\u001b[32m▎         \u001b[0m| 6/200 [00:18<08:01,  2.48s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   4%|\u001b[32m▎         \u001b[0m| 7/200 [00:20<06:42,  2.08s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6.\t Total loss: 1.9045217037200928\n",
      "Validation loss: 3.114173412322998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   4%|\u001b[32m▍         \u001b[0m| 8/200 [00:21<06:11,  1.93s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   4%|\u001b[32m▍         \u001b[0m| 9/200 [00:23<05:29,  1.73s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8.\t Total loss: 1.5855319499969482\n",
      "Validation loss: 2.673637866973877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   5%|\u001b[32m▌         \u001b[0m| 10/200 [00:24<05:00,  1.58s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   5%|\u001b[32m▌         \u001b[0m| 10/200 [00:25<05:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10.\t Total loss: 1.5780181884765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   6%|\u001b[32m▌         \u001b[0m| 11/200 [00:26<05:39,  1.80s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.5508744716644287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   6%|\u001b[32m▌         \u001b[0m| 12/200 [00:27<05:07,  1.64s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   6%|\u001b[32m▋         \u001b[0m| 13/200 [00:29<04:45,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12.\t Total loss: 1.4161946773529053\n",
      "Validation loss: 2.5389771461486816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   7%|\u001b[32m▋         \u001b[0m| 14/200 [00:30<04:30,  1.45s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   8%|\u001b[32m▊         \u001b[0m| 15/200 [00:32<04:43,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14.\t Total loss: 1.354885220527649\n",
      "Validation loss: 2.4540462493896484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   8%|\u001b[32m▊         \u001b[0m| 16/200 [00:34<05:00,  1.63s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:   8%|\u001b[32m▊         \u001b[0m| 17/200 [00:35<04:39,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16.\t Total loss: 1.3200551271438599\n",
      "Validation loss: 2.431151866912842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   9%|\u001b[32m▉         \u001b[0m| 18/200 [00:36<04:25,  1.46s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  10%|\u001b[32m▉         \u001b[0m| 19/200 [00:37<04:13,  1.40s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 18.\t Total loss: 1.2759894132614136\n",
      "Validation loss: 2.424123764038086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  10%|\u001b[32m█         \u001b[0m| 20/200 [00:39<04:33,  1.52s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  10%|\u001b[32m█         \u001b[0m| 20/200 [00:40<04:33,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 20.\t Total loss: 1.422791600227356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  10%|\u001b[32m█         \u001b[0m| 21/200 [00:41<04:53,  1.64s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.4473376274108887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  11%|\u001b[32m█         \u001b[0m| 22/200 [00:42<04:32,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  12%|\u001b[32m█▏        \u001b[0m| 23/200 [00:44<04:16,  1.45s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 22.\t Total loss: 1.4753053188323975\n",
      "Validation loss: 2.6073477268218994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  12%|\u001b[32m█▏        \u001b[0m| 24/200 [00:45<04:06,  1.40s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  12%|\u001b[32m█▎        \u001b[0m| 25/200 [00:47<04:30,  1.54s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 24.\t Total loss: 1.2055549621582031\n",
      "Validation loss: 2.560199499130249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  13%|\u001b[32m█▎        \u001b[0m| 26/200 [00:49<04:46,  1.64s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  14%|\u001b[32m█▎        \u001b[0m| 27/200 [00:50<04:24,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 26.\t Total loss: 1.1416780948638916\n",
      "Validation loss: 2.7062184810638428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  14%|\u001b[32m█▍        \u001b[0m| 28/200 [00:51<04:09,  1.45s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  14%|\u001b[32m█▍        \u001b[0m| 29/200 [00:53<03:59,  1.40s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 28.\t Total loss: 1.2166639566421509\n",
      "Validation loss: 2.6324667930603027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  15%|\u001b[32m█▌        \u001b[0m| 30/200 [00:54<03:51,  1.36s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  15%|\u001b[32m█▌        \u001b[0m| 30/200 [00:55<03:51,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 30.\t Total loss: 1.00675368309021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  16%|\u001b[32m█▌        \u001b[0m| 31/200 [00:56<04:18,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.6211204528808594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  16%|\u001b[32m█▌        \u001b[0m| 32/200 [00:58<04:40,  1.67s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  16%|\u001b[32m█▋        \u001b[0m| 33/200 [00:59<04:19,  1.55s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 32.\t Total loss: 0.9383493065834045\n",
      "Validation loss: 2.6707041263580322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  17%|\u001b[32m█▋        \u001b[0m| 34/200 [01:00<04:04,  1.47s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  18%|\u001b[32m█▊        \u001b[0m| 35/200 [01:02<03:53,  1.41s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 34.\t Total loss: 1.0617785453796387\n",
      "Validation loss: 2.717081308364868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  18%|\u001b[32m█▊        \u001b[0m| 36/200 [01:03<04:15,  1.56s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  18%|\u001b[32m█▊        \u001b[0m| 37/200 [01:05<04:00,  1.47s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 36.\t Total loss: 0.9397204518318176\n",
      "Validation loss: 2.8087923526763916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  19%|\u001b[32m█▉        \u001b[0m| 38/200 [01:06<03:48,  1.41s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  20%|\u001b[32m█▉        \u001b[0m| 39/200 [01:08<04:22,  1.63s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 38.\t Total loss: 0.6182915568351746\n",
      "Validation loss: 2.8268017768859863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  20%|\u001b[32m██        \u001b[0m| 40/200 [01:09<04:05,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  20%|\u001b[32m██        \u001b[0m| 40/200 [01:11<04:05,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 40.\t Total loss: 0.7503646016120911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  20%|\u001b[32m██        \u001b[0m| 41/200 [01:11<04:21,  1.64s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.985948324203491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  21%|\u001b[32m██        \u001b[0m| 42/200 [01:13<04:03,  1.54s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  22%|\u001b[32m██▏       \u001b[0m| 43/200 [01:14<03:49,  1.46s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 42.\t Total loss: 0.42483845353126526\n",
      "Validation loss: 2.7881033420562744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  22%|\u001b[32m██▏       \u001b[0m| 44/200 [01:15<03:40,  1.42s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  22%|\u001b[32m██▎       \u001b[0m| 45/200 [01:17<03:33,  1.38s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 44.\t Total loss: 0.39458152651786804\n",
      "Validation loss: 2.9418280124664307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  23%|\u001b[32m██▎       \u001b[0m| 46/200 [01:18<03:55,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  24%|\u001b[32m██▎       \u001b[0m| 47/200 [01:21<04:26,  1.74s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 46.\t Total loss: 0.32970401644706726\n",
      "Validation loss: 3.051959991455078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  24%|\u001b[32m██▍       \u001b[0m| 48/200 [01:22<04:03,  1.60s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  24%|\u001b[32m██▍       \u001b[0m| 49/200 [01:23<03:47,  1.51s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 48.\t Total loss: 0.26353052258491516\n",
      "Validation loss: 2.8342785835266113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  25%|\u001b[32m██▌       \u001b[0m| 50/200 [01:24<03:37,  1.45s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  25%|\u001b[32m██▌       \u001b[0m| 50/200 [01:26<03:37,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 50.\t Total loss: 0.24807588756084442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  26%|\u001b[32m██▌       \u001b[0m| 51/200 [01:26<03:55,  1.58s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.9387919902801514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  26%|\u001b[32m██▌       \u001b[0m| 52/200 [01:28<03:40,  1.49s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  26%|\u001b[32m██▋       \u001b[0m| 53/200 [01:29<03:29,  1.42s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 52.\t Total loss: 0.20441143214702606\n",
      "Validation loss: 3.085800886154175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  27%|\u001b[32m██▋       \u001b[0m| 54/200 [01:30<03:22,  1.38s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  28%|\u001b[32m██▊       \u001b[0m| 55/200 [01:32<03:16,  1.36s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 54.\t Total loss: 0.1526268869638443\n",
      "Validation loss: 2.9975714683532715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  28%|\u001b[32m██▊       \u001b[0m| 56/200 [01:33<03:38,  1.52s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  28%|\u001b[32m██▊       \u001b[0m| 57/200 [01:36<04:17,  1.80s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 56.\t Total loss: 0.14306724071502686\n",
      "Validation loss: 2.9275381565093994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  29%|\u001b[32m██▉       \u001b[0m| 58/200 [01:37<03:54,  1.65s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  30%|\u001b[32m██▉       \u001b[0m| 59/200 [01:38<03:36,  1.54s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 58.\t Total loss: 0.5850251317024231\n",
      "Validation loss: 3.0370869636535645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  30%|\u001b[32m███       \u001b[0m| 60/200 [01:40<03:24,  1.46s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  30%|\u001b[32m███       \u001b[0m| 60/200 [01:41<03:24,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 60.\t Total loss: 0.130988210439682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  30%|\u001b[32m███       \u001b[0m| 61/200 [01:42<03:42,  1.60s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.032902240753174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  31%|\u001b[32m███       \u001b[0m| 62/200 [01:43<03:27,  1.51s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  32%|\u001b[32m███▏      \u001b[0m| 63/200 [01:44<03:17,  1.44s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 62.\t Total loss: 0.12893785536289215\n",
      "Validation loss: 2.9384260177612305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  32%|\u001b[32m███▏      \u001b[0m| 64/200 [01:46<03:09,  1.39s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  32%|\u001b[32m███▎      \u001b[0m| 65/200 [01:47<03:04,  1.36s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 64.\t Total loss: 0.13449157774448395\n",
      "Validation loss: 3.015401840209961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  33%|\u001b[32m███▎      \u001b[0m| 66/200 [01:49<03:25,  1.53s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Entrenando:  34%|\u001b[32m███▎      \u001b[0m| 67/200 [01:50<03:14,  1.46s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 66.\t Total loss: 0.3706081807613373\n",
      "Validation loss: 3.116382360458374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  34%|\u001b[32m███▍      \u001b[0m| 68/200 [01:51<03:05,  1.41s/it]('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\", \"L['self'].param_groups[0]['params'][44].grad\", \"L['self'].param_groups[0]['params'][45].grad\", \"L['self'].param_groups[0]['params'][46].grad\", \"L['self'].param_groups[0]['params'][47].grad\", \"L['self'].param_groups[0]['params'][48].grad\", \"L['self'].param_groups[0]['params'][49].grad\", \"L['self'].param_groups[0]['params'][50].grad\", \"L['self'].param_groups[0]['params'][51].grad\", \"L['self'].param_groups[0]['params'][52].grad\", \"L['self'].param_groups[0]['params'][53].grad\", \"L['self'].param_groups[0]['params'][54].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x766682e6c7c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/giorgio6846/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 589, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Entrenando:  34%|\u001b[32m███▍      \u001b[0m| 68/200 [01:54<03:05,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 68.\t Total loss: 0.7478004097938538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  34%|\u001b[32m███▍      \u001b[0m| 68/200 [01:59<03:51,  1.75s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1251237, 1251238, 1251239, 1251240) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_config\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      3\u001b[0m })\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-chris/src/mslm/utils/setup_train.py:114\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(params, train_dataloader, val_dataloader, model, profile_model)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/nvtx/nvtx.py:122\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 122\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-chris/src/mslm/training/trainer.py:103\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, prof)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_mgr\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, epoch)\n\u001b[0;32m--> 103\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# if self.scheduler is not None:\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#     self.scheduler.step()\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping\u001b[38;5;241m.\u001b[39mstop:\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/nvtx/nvtx.py:122\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 122\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-chris/src/mslm/training/trainer.py:228\u001b[0m, in \u001b[0;36mTrainer._val\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    226\u001b[0m val_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeypoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:488\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1230\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1228\u001b[0m resume_iteration_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1230\u001b[0m     return_idx, return_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_ResumeIteration):\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1251237, 1251238, 1251239, 1251240) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "train_config.update({\n",
    "  \"epochs\": 200,\n",
    "})\n",
    "run_training(train_config, tr_dl, val_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1251237, 1251238, 1251239, 1251240) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:488\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1230\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1228\u001b[0m resume_iteration_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1230\u001b[0m     return_idx, return_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_ResumeIteration):\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1251237, 1251238, 1251239, 1251240) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dl:\n",
    "        inputs, mask, targets, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        mask = mask.to(device)\n",
    "        outputs = model(inputs, mask)\n",
    "        print(f\"Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}, Targets shape: {targets.shape}\")\n",
    "        break  # Solo para probar el primer batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 2.086271286010742\n"
     ]
    }
   ],
   "source": [
    "L_common = min(outputs.size(1), targets.size(1))\n",
    "pred_embs     = outputs   [:, :L_common]\n",
    "target_embs   = targets [:, :L_common]\n",
    "embedding_mask = mask[:, :L_common]\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(pred_embs, target_embs)\n",
    "print(\"MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0897, -0.3952, -0.1157,  ...,  0.4509,  0.3203,  0.3035],\n",
       "         [ 0.9322, -1.1671, -0.2401,  ...,  0.9682, -0.9013,  0.0350],\n",
       "         [ 0.1231, -0.4041, -0.9660,  ..., -0.4154, -2.0379,  1.2492],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0804, -0.4052, -0.1248,  ...,  0.4500,  0.3122,  0.2966],\n",
       "         [ 0.0402, -2.1052,  1.2279,  ...,  1.7971, -0.5469, -0.7472],\n",
       "         [-1.6225, -1.8591, -0.6591,  ..., -1.4079, -0.5445, -0.8075],\n",
       "         ...,\n",
       "         [-1.2914,  1.0705,  1.3059,  ..., -0.6800, -0.3671,  2.2190],\n",
       "         [-0.8618,  1.6748,  2.6077,  ..., -0.8423,  0.7961,  1.5343],\n",
       "         [ 0.2764, -1.0429, -2.7705,  ...,  0.1155, -0.4705, -0.9378]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1389, -0.5081, -0.3362,  ...,  0.3684,  0.2839,  0.4170],\n",
       "         [-2.2625, -0.1232,  1.0554,  ...,  0.7423, -1.6623, -0.2631],\n",
       "         [-0.7219,  1.0109,  0.8119,  ..., -0.0616, -3.9642, -1.5129],\n",
       "         ...,\n",
       "         [-1.9372, -0.5624,  0.1327,  ...,  0.5759, -2.5311, -0.8052],\n",
       "         [-0.2607, -0.8605,  1.0719,  ...,  0.3578, -0.7986, -0.0767],\n",
       "         [-1.3852, -0.7967,  0.2694,  ...,  0.5442, -1.5376, -1.2053]],\n",
       "\n",
       "        [[-0.1372, -0.6028, -0.4260,  ...,  0.2968,  0.3841,  0.3483],\n",
       "         [-2.1371, -0.1450,  0.8849,  ...,  0.6219, -1.4993, -0.1890],\n",
       "         [-0.6464,  1.0247,  0.7741,  ..., -0.1103, -3.8964, -1.4245],\n",
       "         ...,\n",
       "         [-2.0894, -0.5207, -0.0071,  ...,  0.2462, -2.8643, -0.8986],\n",
       "         [-0.0597, -0.9253,  1.1884,  ...,  0.0332, -0.6494, -0.0531],\n",
       "         [-0.9783, -0.8845, -0.0953,  ...,  0.4618, -1.0861, -1.2029]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1389, -0.5081, -0.3362,  ...,  0.3684,  0.2839,  0.4170],\n",
       "         [-2.2625, -0.1232,  1.0554,  ...,  0.7423, -1.6623, -0.2631],\n",
       "         [-0.7219,  1.0109,  0.8119,  ..., -0.0616, -3.9642, -1.5129],\n",
       "         ...,\n",
       "         [-1.4484, -1.0098, -0.1699,  ..., -0.2217, -0.6729,  0.2320],\n",
       "         [-0.4084, -0.0734, -0.1730,  ...,  0.3982, -0.6137,  0.0183],\n",
       "         [-1.7066, -0.0720,  0.0958,  ...,  0.3263, -1.3347, -0.3726]],\n",
       "\n",
       "        [[-0.1372, -0.6028, -0.4260,  ...,  0.2968,  0.3841,  0.3483],\n",
       "         [-2.1371, -0.1450,  0.8849,  ...,  0.6219, -1.4993, -0.1890],\n",
       "         [-0.6464,  1.0247,  0.7741,  ..., -0.1103, -3.8964, -1.4245],\n",
       "         ...,\n",
       "         [-1.6019, -0.9580, -0.2105,  ..., -0.2810, -0.2262,  0.0546],\n",
       "         [-0.4252,  0.0775, -0.3439,  ...,  0.4071, -0.1987, -0.2586],\n",
       "         [-1.8843,  0.0857,  0.0350,  ...,  0.2321, -1.1898, -0.6596]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
